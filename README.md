# ğŸš— Proyecto ETL sobre Precios de Combustibles en MÃ©xico â›½

Este proyecto es un pipeline ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) diseÃ±ado para obtener, procesar y almacenar datos sobre los precios de combustibles en MÃ©xico. Utiliza Apache Airflow para la orquestaciÃ³n ğŸ”„ y Apache Spark para el procesamiento de los datos âš¡. Los datos provienen de fuentes oficiales disponibles en el portal de Datos Abiertos de MÃ©xico ğŸŒ (https://datos.gob.mx/).

## CaracterÃ­sticas ğŸŒŸ
- **Fuente de Datos**: Los datos provienen de la ComisiÃ³n Reguladora de EnergÃ­a (CRE) ğŸ“Š, los cuales se actualizan periÃ³dicamente y estÃ¡n disponibles para consulta pÃºblica.
- **Ingesta**: Se extraen los datos en formato XML desde el portal de Datos Abiertos de MÃ©xico ğŸ“¥, que proporcionan los precios actuales de combustibles.
- **TransformaciÃ³n**: Se realiza la limpieza y estructuraciÃ³n de los datos mediante Apache Spark ğŸ”§ para asegurar su calidad y facilidad de anÃ¡lisis.
- **Almacenamiento**: Los datos procesados se cargan en AWS Redshift ğŸ“¦, una base de datos en la nube optimizada para el anÃ¡lisis de grandes volÃºmenes de datos.
- **OrquestaciÃ³n**: Apache Airflow se encarga de gestionar la ejecuciÃ³n del pipeline ETL â›…, garantizando la automatizaciÃ³n y la programaciÃ³n de las tareas dentro de contenedores Docker ğŸ³.

A continuaciÃ³n se muestra el diagrama del pipeline:

![Diagram](https://github.com/JuanCarlosOP03/fuel-prices-etl-mexico/blob/main/diagram.jpg)


## ğŸ“Š Modelo de Datos

El modelo de datos de este proyecto estÃ¡ compuesto por tres tablas principales que almacenan informaciÃ³n relacionada con las estaciones de combustible, los precios y los detalles de las estaciones. A continuaciÃ³n, se describen las tablas y sus columnas, junto con las fuentes de donde provienen los datos.

### 1. **Tabla `places`** ğŸª
Esta tabla almacena la informaciÃ³n bÃ¡sica sobre las estaciones de servicio.

```sql
CREATE TABLE "mx_prices_fuel"."places" (
    "place_id" INT PRIMARY KEY NOT NULL,
    "cre_id" VARCHAR(25) NOT NULL,
    "longitude" DOUBLE PRECISION NOT NULL, 
    "latitude" DOUBLE PRECISION NOT NULL,  
    "place_name" VARCHAR(150)                
);
```

- **place_id**: Identificador Ãºnico de la estaciÃ³n de servicio.
- **cre_id**: Identificador del permiso.
- **longitude**: Longitud de la estaciÃ³n de servicio en grados.
- **latitude**: Latitud de la estaciÃ³n de servicio en grados.
- **place_name**: Nombre de la estaciÃ³n de servicio.

**Fuente**: [Datos sobre estaciones de servicio](https://publicacionexterna.azurewebsites.net/publicaciones/places)

---

### 2. **Tabla `prices`** ğŸ’°
Esta tabla almacena los precios de los combustibles en las estaciones de servicio.

```sql
CREATE TABLE "mx_prices_fuel"."prices" (
    "place_id" INT NOT NULL REFERENCES "mx_prices_fuel"."places" ("place_id"),  
    "fuel_type" VARCHAR(30) NOT NULL,
    "type_product" VARCHAR(30) NOT NULL,
    "price" DOUBLE PRECISION NOT NULL
);
```

- **place_id**: Referencia a la estaciÃ³n de servicio.
- **fuel_type**: Tipo de combustible (por ejemplo, gasolina o diÃ©sel).
- **type_product**: Tipo de producto (por ejemplo, premium, magna, o diÃ©sel).
- **price**: Precio del combustible en la estaciÃ³n de servicio.

**Fuente**: [Precios de combustibles](https://publicacionexterna.azurewebsites.net/publicaciones/prices)

---

### 3. **Tabla `places_details`** ğŸ 
Esta tabla contiene detalles adicionales sobre las estaciones de servicio, como su ubicaciÃ³n y fecha de entrada.

```sql
CREATE TABLE "mx_prices_fuel"."places_details" (
    "turn_code" VARCHAR(10) NOT NULL,
    "cre_id" VARCHAR(25) NOT NULL,
    "place_name" VARCHAR(150),
    "place_code" VARCHAR(10) NOT NULL,
    "date_entry" DATE,
    "plenary_date" DATE,
    "address" VARCHAR(200),
    "colony" VARCHAR(100),
    "cp" INT,
    "city" VARCHAR(80) NOT NULL,
    "state" VARCHAR(50) NOT NULL
);
```

- **turn_code**: CÃ³digo del turno de la estaciÃ³n de servicio.
- **cre_id**: Identificador del permiso.
- **place_name**: Nombre de la estaciÃ³n de servicio.
- **place_code**: CÃ³digo del lugar.
- **date_entry**: Fecha de entrada en la base de datos.
- **plenary_date**: Fecha plenaria.
- **address**: DirecciÃ³n de la estaciÃ³n de servicio.
- **colony**: Colonia de la estaciÃ³n de servicio.
- **cp**: CÃ³digo postal de la estaciÃ³n de servicio.
- **city**: Ciudad de la estaciÃ³n de servicio.
- **state**: Estado de la estaciÃ³n de servicio.

**Fuente**: [Informe de Expendio en Estaciones de Servicio - CRE](https://www.cre.gob.mx/documento/Expendioenestacionesdeservici.pdf)


## TecnologÃ­as Utilizadas ğŸ–¥ï¸
- **Docker & Docker Compose** ğŸ³: Para la contenerizaciÃ³n de los servicios y su configuraciÃ³n.
- **Apache Airflow** ğŸŒ¬ï¸: Para la gestiÃ³n y orquestaciÃ³n de las tareas del pipeline ETL.
- **Apache Spark** âš¡: Para el procesamiento y transformaciÃ³n de los datos.
- **AWS Redshift** ğŸŒ: Para el almacenamiento de los datos procesados.
- **Python** ğŸ: Lenguaje utilizado para la implementaciÃ³n de la lÃ³gica ETL.

## InstalaciÃ³n y ConfiguraciÃ³n âš™ï¸
### Prerrequisitos ğŸ“‹
AsegÃºrate de tener los siguientes requisitos instalados:
- Docker y Docker Compose ğŸ³
- Python 3.x ğŸ

### Pasos de instalaciÃ³n ğŸ› ï¸
1. Clona el repositorio:
   ```bash
   export LOCAL_USER="$(whoami)"
   sudo mkdir /opt/airflow
   sudo chown -R $LOCAL_USER:root /opt/airflow
   chmod g+w /opt/airflow

   git clone https://github.com/JuanCarlosOP03/fuel-prices-etl-mexico.git /opt/airflow

   cd /opt/airflow
   
   source ./setup.sh
   ```
2. Configura las variables de entorno en `config/airflow.env`. AsegÃºrate de completar las siguientes variables:
   ```
   AWS_REGION=
   AWS_ACCESS_KEY_ID=
   AWS_SECRET_ACCESS_KEY=
   AWS_S3_ENDPOINT=
   ```
3. Construye y levanta los contenedores:
   ```bash
   docker-compose up -d --build
   ```
4. Accede a la interfaz de Airflow en `http://localhost:8080` y activa el DAG ğŸ”„.
5. Crea las siguientes conexiones en Airflow para asegurar el funcionamiento del pipeline:
   - `aws_default` ğŸŒ
   - `spark_default` âš¡
   - `redshift_default` ğŸ’¾

Se recomienda almacenar el proyecto en `/opt/spark` para un rendimiento Ã³ptimo ğŸš€.

## Uso ğŸ› ï¸
- La ejecuciÃ³n del pipeline se gestiona desde la interfaz de Airflow ğŸ–¥ï¸.
- Los datos procesados y transformados se cargan en la base de datos Redshift y estÃ¡n listos para anÃ¡lisis posteriores ğŸ“Š.

## Estructura del Proyecto ğŸ“
```
â”œâ”€â”€ Dockerfile.Airflow
â”œâ”€â”€ Dockerfile.Spark
â”œâ”€â”€ README.md
â”œâ”€â”€ bd
â”‚   â””â”€â”€ crate_tables.sql
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ airflow.env
â”‚   â”œâ”€â”€ spark-defaults.conf
â”‚   â””â”€â”€ spark.env
â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ pyspark_scripts
â”‚   â”‚   â”œâ”€â”€ process_places.py
â”‚   â”‚   â”œâ”€â”€ process_places_details.py
â”‚   â”‚   â””â”€â”€ process_prices.py
â”‚   â”œâ”€â”€ scr
â”‚   â”‚   â”œâ”€â”€ config
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ conf.py
â”‚   â”‚   â””â”€â”€ utils
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ extractor.py
â”‚   â””â”€â”€ test.py
â”œâ”€â”€ data
â”‚   â””â”€â”€ data
â”‚       â””â”€â”€ ext
â”‚           â”œâ”€â”€ places.xml
â”‚           â”œâ”€â”€ places_detail.pdf
â”‚           â””â”€â”€ prices.xml
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ jars
â”‚   â”œâ”€â”€ aws-java-sdk-bundle-1.12.262.jar
â”‚   â”œâ”€â”€ aws-java-sdk-core-1.12.262.jar
â”‚   â”œâ”€â”€ hadoop-aws-3.3.4.jar
â”‚   â”œâ”€â”€ postgresql-42.7.4.jar
â”‚   â””â”€â”€ spark-xml_2.12-0.14.0.jar
â”œâ”€â”€ logs
â”œâ”€â”€ plugins
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.sh
```
